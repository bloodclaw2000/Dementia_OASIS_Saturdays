{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOVFPK6ecbsD1HKNSYhxD2A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bloodclaw2000/Dementia_OASIS_Saturdays/blob/main/dementia%20colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forzamos una versión de Python más nueva y clonamos el repositorio de GitHub para los archivos python"
      ],
      "metadata": {
        "id": "1J4J29PUzc0k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rww1GjjtoynD",
        "outputId": "74302c3c-ab0d-42a6-9aa0-2d1f2c7de57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Dementia_OASIS_Saturdays'...\n",
            "remote: Enumerating objects: 69, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 69 (delta 0), reused 0 (delta 0), pack-reused 66\u001b[K\n",
            "Receiving objects: 100% (69/69), 135.34 MiB | 25.32 MiB/s, done.\n",
            "Resolving deltas: 100% (18/18), done.\n",
            "/content/Dementia_OASIS_Saturdays\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/usr/local/lib/python3.11/dist-packages/') #We need to upgrade Python Client for some reason COLAB is py 3.6\n",
        "\n",
        "#!python -m pip install --upgrade pip\n",
        "!git clone https://github.com/bloodclaw2000/Dementia_OASIS_Saturdays.git\n",
        "%cd Dementia_OASIS_Saturdays"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos las carpetas para guardar las redes neuronales, los datasets y las figuras"
      ],
      "metadata": {
        "id": "4iq5tFawzjtr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%mkdir nn\n",
        "%mkdir dataset\n",
        "%mkdir plots"
      ],
      "metadata": {
        "id": "wDeaDgqbspmE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos las librerias"
      ],
      "metadata": {
        "id": "ToL4p8pJzxQ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import sys\n",
        "from scipy.signal import convolve2d\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import _pickle as cPickle\n",
        "import bz2\n",
        "import csv\n",
        "import torch\n",
        "import torchvision\n",
        "from torch.utils.data import Dataset\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "from torchvision import transforms\n",
        "from tabulate import tabulate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
        "from IPython.display import Image\n",
        "from sklearn import tree"
      ],
      "metadata": {
        "id": "hB0gXoFvr-1c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importamos las funciones necesarias desde nuestros programas python"
      ],
      "metadata": {
        "id": "ak2X7spQzzlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pickle_aux import pet_load, decompress_pickle, pet_save\n",
        "from dementia_network_class import Dementia, train_nn, getOutput\n",
        "from dementia_tree_class import train_tree,  Customtree\n",
        "\n"
      ],
      "metadata": {
        "id": "tnCzQkXjpWlE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usamos CPU o GPU dependiendo de lo que haya"
      ],
      "metadata": {
        "id": "rlgOPJiaz3BY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.multiprocessing.set_start_method('spawn', force=True) #kinda important mostly for CPU\n",
        "\n",
        "device = torch.device(\n",
        "                                        f'cuda:{torch.cuda.current_device()}'\n",
        "                                        if torch.cuda.is_available()\n",
        "                                        else 'cpu')"
      ],
      "metadata": {
        "id": "mebdeb92sXOj"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos el diccionario y generamos uno nuevo limpio sin comprimir"
      ],
      "metadata": {
        "id": "COByid_Kz7g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compressed_pickle_directory = \"save_dict3\"\n",
        "if not os.path.exists('{0}_decompressed.p'.format(compressed_pickle_directory)):\n",
        "    def force_dementia(dictionary):\n",
        "              for key in dictionary:\n",
        "                        for key2 in dictionary[key]:\n",
        "                            if dictionary[key][key2]['CDR'] == '':\n",
        "                                      dictionary[key][key2]['Dementia'] = 0\n",
        "                            elif float(dictionary[key][key2]['CDR']) > 0:\n",
        "                                      dictionary[key][key2]['Dementia'] = 1\n",
        "                            else:\n",
        "                                      dictionary[key][key2]['Dementia'] = 0\n",
        "              return dictionary\n",
        "\n",
        "    def removeyoung(dictionary, age):\n",
        "              dic_pacientes_viejos = {}\n",
        "              for key in dictionary:\n",
        "                        for key2 in dictionary[key]:\n",
        "                            if int(dictionary[key][key2]['Age']) >= age:\n",
        "                                      dic_pacientes_viejos[key] = dictionary[key]\n",
        "              return dic_pacientes_viejos\n",
        "\n",
        "    tmp_dict = decompress_pickle('{0}.pbz2'.format(compressed_pickle_directory))\n",
        "    tmp_dict = force_dementia(tmp_dict) #esto es la funcion del init\n",
        "    tmp_dict = removeyoung(tmp_dict, 59)\n",
        "    tmp_dict = {int(key): value for key, value in tmp_dict.items()}\n",
        "    new_dict = {}\n",
        "    current_index = 0\n",
        "    for key in sorted(tmp_dict.keys()):\n",
        "                        new_dict[current_index] = tmp_dict[key]\n",
        "                        current_index += 1\n",
        "    tmp_dict = new_dict\n",
        "\n",
        "    pet_save(tmp_dict,'{0}_decompressed.p'.format(compressed_pickle_directory))\n",
        "else:\n",
        "     tmp_dict=pet_load('{0}_decompressed.p'.format(compressed_pickle_directory))\n"
      ],
      "metadata": {
        "id": "cseOO1XLpjCr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Forzamos el device y generamos un objeto de todas las redes neuronales"
      ],
      "metadata": {
        "id": "O_d_9dsh0BAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_default_device(device)\n",
        "\n",
        "print(device)\n",
        "\n",
        "print(f\" Using {device} in this run\")\n",
        "\n",
        "obj = Dementia(dictionary=tmp_dict,device=device)\n"
      ],
      "metadata": {
        "id": "cLOP5NcVp7Zb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87edc90c-0025-4616-c8eb-36c3737ca4d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n",
            " Using cuda:0 in this run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Seteamos los parámetros manualmente que queramos, también hay archivos txt en params"
      ],
      "metadata": {
        "id": "6V_fF1Rz0GzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Podemos setear los parámetros para el entrenamiento aquí:\n",
        "obj.setParam('image_type','T88_111')\n",
        "obj.setParam('image_number',1)\n"
      ],
      "metadata": {
        "id": "Vgr8pRKMsKL2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos cada red neuronal con el tipo de imagenes que queramos y creamos los datasets"
      ],
      "metadata": {
        "id": "f__rKWY70LME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train(obj,'params/',['T88'])\n",
        "# train(obj,'params/',['T88','FSL'])\n",
        "train_nn(obj, 'params_nn/', ['T88', 'FSL', 'RAW_1', 'RAW_2', 'RAW_3'])\n",
        "\n",
        "# getOutput('nn/',['T88'])\n",
        "# getOutput('nn/',['T88','FSL'])\n",
        "getOutput('nn/', ['T88', 'FSL', 'RAW_1', 'RAW_2', 'RAW_3'])\n"
      ],
      "metadata": {
        "id": "LjddxO7_sLmO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feddbf02-263d-46f1-ae7e-a210fb667643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read parameters dictionary at relative location params_nn/RAW_2.txt and setting them \n",
            "Param invalid_key does not exist\n",
            "Param does not exist\n",
            "Resetting to default value\n",
            "Now calculating predictions for file params_nn/RAW_2.txt\n",
            "{'plot': 'True', 'image_type': 'RAW', 'image_number': 2, 'patience_validation': 3, 'patience_plateau': 3, 'validation_patience': 3, 'delta_min': 0, 'batch_size': 10, 'split_size': 0.8, 'max_loss_reset': 5, 'learning_rate': 0.0001, 'weight_decay': 0.1, 'nepochs': 1000, 'first_conv_outchann': 6, 'second_conv_outchann': 16, 'fclayer1': 120, 'fclayer2': 'None', 'criterion_type': 'BCElogitsloss', 'optimizer': 'Adam'}\n",
            "[1] tr loss: 49.984\n",
            "[1] va loss: 0.828\n",
            "83 78\n",
            "[2] tr loss: 22.560\n",
            "[2] va loss: 1.387\n",
            "83 78\n",
            "[3] tr loss: 11.789\n",
            "[3] va loss: 0.644\n",
            "83 78\n",
            "[4] tr loss: 6.743\n",
            "[4] va loss: 1.042\n",
            "83 78\n",
            "[5] tr loss: 6.134\n",
            "[5] va loss: 0.674\n",
            "83 78\n",
            "[6] tr loss: 4.141\n",
            "[6] va loss: 0.686\n",
            "83 78\n",
            "[7] tr loss: 2.790\n",
            "[7] va loss: 0.749\n",
            "83 78\n",
            "Trying to force it out of a plateau\n",
            "[8] tr loss: 2.223\n",
            "[8] va loss: 0.698\n",
            "83 78\n",
            "[9] tr loss: 2.471\n",
            "[9] va loss: 0.992\n",
            "83 78\n",
            "[10] tr loss: 1.775\n",
            "[10] va loss: 0.736\n",
            "83 78\n",
            "[11] tr loss: 5.011\n",
            "[11] va loss: 1.060\n",
            "83 78\n",
            "SHOULD STOP EARLY BUT ERROR TOO HIGH\n",
            "Resetting Lineal Coefficients\n",
            "[12] tr loss: 2691.101\n",
            "[12] va loss: 51.979\n",
            "83 78\n",
            "[13] tr loss: 248.398\n",
            "[13] va loss: 16.839\n",
            "83 78\n",
            "[14] tr loss: 101.267\n",
            "[14] va loss: 7.328\n",
            "83 78\n",
            "[15] tr loss: 11.812\n",
            "[15] va loss: 5.800\n",
            "83 78\n",
            "[16] tr loss: 1.311\n",
            "[16] va loss: 6.338\n",
            "83 78\n",
            "[17] tr loss: 0.797\n",
            "[17] va loss: 4.899\n",
            "83 78\n",
            "[18] tr loss: 0.070\n",
            "[18] va loss: 4.882\n",
            "83 78\n",
            "[19] tr loss: 0.002\n",
            "[19] va loss: 4.881\n",
            "83 78\n",
            "[20] tr loss: 0.001\n",
            "[20] va loss: 4.872\n",
            "83 78\n",
            "[21] tr loss: 0.001\n",
            "[21] va loss: 4.859\n",
            "83 78\n",
            "[22] tr loss: 0.001\n",
            "[22] va loss: 4.845\n",
            "83 78\n",
            "[23] tr loss: 0.001\n",
            "[23] va loss: 4.832\n",
            "83 78\n",
            "[24] tr loss: 0.001\n",
            "[24] va loss: 4.819\n",
            "83 78\n",
            "[25] tr loss: 0.001\n",
            "[25] va loss: 4.805\n",
            "83 78\n",
            "[26] tr loss: 0.001\n",
            "[26] va loss: 4.792\n",
            "83 78\n",
            "[27] tr loss: 0.001\n",
            "[27] va loss: 4.779\n",
            "83 78\n",
            "[28] tr loss: 0.001\n",
            "[28] va loss: 4.765\n",
            "83 78\n",
            "[29] tr loss: 0.001\n",
            "[29] va loss: 4.752\n",
            "83 78\n",
            "[30] tr loss: 0.002\n",
            "[30] va loss: 4.739\n",
            "83 78\n",
            "[31] tr loss: 0.001\n",
            "[31] va loss: 4.722\n",
            "83 78\n",
            "[32] tr loss: 0.000\n",
            "[32] va loss: 4.709\n",
            "83 78\n",
            "[33] tr loss: 0.000\n",
            "[33] va loss: 4.697\n",
            "83 78\n",
            "[34] tr loss: 0.000\n",
            "[34] va loss: 4.684\n",
            "83 78\n",
            "[35] tr loss: 0.000\n",
            "[35] va loss: 4.672\n",
            "83 78\n",
            "[36] tr loss: 0.000\n",
            "[36] va loss: 4.660\n",
            "83 78\n",
            "[37] tr loss: 0.000\n",
            "[37] va loss: 4.648\n",
            "83 78\n",
            "[38] tr loss: 0.000\n",
            "[38] va loss: 4.636\n",
            "83 78\n",
            "[39] tr loss: 0.000\n",
            "[39] va loss: 4.625\n",
            "83 78\n",
            "[40] tr loss: 0.000\n",
            "[40] va loss: 4.613\n",
            "83 78\n",
            "[41] tr loss: 0.001\n",
            "[41] va loss: 4.601\n",
            "83 78\n",
            "[42] tr loss: 0.001\n",
            "[42] va loss: 4.591\n",
            "83 78\n",
            "[43] tr loss: 0.001\n",
            "[43] va loss: 4.580\n",
            "83 78\n",
            "[44] tr loss: 0.001\n",
            "[44] va loss: 4.569\n",
            "83 78\n",
            "[45] tr loss: 0.001\n",
            "[45] va loss: 4.558\n",
            "83 78\n",
            "[46] tr loss: 0.001\n",
            "[46] va loss: 4.548\n",
            "83 78\n",
            "[47] tr loss: 0.001\n",
            "[47] va loss: 4.538\n",
            "83 78\n",
            "[48] tr loss: 0.001\n",
            "[48] va loss: 4.528\n",
            "83 78\n",
            "[49] tr loss: 0.001\n",
            "[49] va loss: 4.517\n",
            "83 78\n",
            "[50] tr loss: 0.001\n",
            "[50] va loss: 4.508\n",
            "83 78\n",
            "[51] tr loss: 0.001\n",
            "[51] va loss: 4.497\n",
            "83 78\n",
            "[52] tr loss: 0.001\n",
            "[52] va loss: 4.488\n",
            "83 78\n",
            "[53] tr loss: 0.001\n",
            "[53] va loss: 4.478\n",
            "83 78\n",
            "[54] tr loss: 0.001\n",
            "[54] va loss: 4.468\n",
            "83 78\n",
            "[55] tr loss: 0.001\n",
            "[55] va loss: 4.460\n",
            "83 78\n",
            "[56] tr loss: 0.001\n",
            "[56] va loss: 4.451\n",
            "83 78\n",
            "[57] tr loss: 0.001\n",
            "[57] va loss: 4.442\n",
            "83 78\n",
            "[58] tr loss: 0.001\n",
            "[58] va loss: 4.432\n",
            "83 78\n",
            "[59] tr loss: 0.001\n",
            "[59] va loss: 4.423\n",
            "83 78\n",
            "[60] tr loss: 0.001\n",
            "[60] va loss: 4.412\n",
            "83 78\n",
            "[61] tr loss: 0.001\n",
            "[61] va loss: 4.403\n",
            "83 78\n",
            "[62] tr loss: 0.001\n",
            "[62] va loss: 4.392\n",
            "83 78\n",
            "[63] tr loss: 0.001\n",
            "[63] va loss: 4.382\n",
            "83 78\n",
            "[64] tr loss: 0.001\n",
            "[64] va loss: 4.374\n",
            "83 78\n",
            "[65] tr loss: 0.001\n",
            "[65] va loss: 4.364\n",
            "83 78\n",
            "[66] tr loss: 0.001\n",
            "[66] va loss: 4.352\n",
            "83 78\n",
            "[67] tr loss: 0.001\n",
            "[67] va loss: 4.342\n",
            "83 78\n",
            "[68] tr loss: 0.001\n",
            "[68] va loss: 4.333\n",
            "83 78\n",
            "[69] tr loss: 0.001\n",
            "[69] va loss: 4.321\n",
            "83 78\n",
            "[70] tr loss: 0.001\n",
            "[70] va loss: 4.312\n",
            "83 78\n",
            "[71] tr loss: 0.001\n",
            "[71] va loss: 4.309\n",
            "83 78\n",
            "[72] tr loss: 0.001\n",
            "[72] va loss: 4.300\n",
            "83 78\n",
            "[73] tr loss: 0.001\n",
            "[73] va loss: 4.288\n",
            "83 78\n",
            "[74] tr loss: 0.001\n",
            "[74] va loss: 4.275\n",
            "83 78\n",
            "[75] tr loss: 0.001\n",
            "[75] va loss: 4.267\n",
            "83 78\n",
            "[76] tr loss: 0.001\n",
            "[76] va loss: 4.256\n",
            "83 78\n",
            "[77] tr loss: 0.001\n",
            "[77] va loss: 4.243\n",
            "83 78\n",
            "[78] tr loss: 0.001\n",
            "[78] va loss: 4.231\n",
            "83 78\n",
            "[79] tr loss: 0.001\n",
            "[79] va loss: 4.220\n",
            "83 78\n",
            "[80] tr loss: 0.002\n",
            "[80] va loss: 4.208\n",
            "83 78\n",
            "[81] tr loss: 0.001\n",
            "[81] va loss: 4.206\n",
            "83 78\n",
            "[82] tr loss: 0.001\n",
            "[82] va loss: 4.192\n",
            "83 78\n",
            "[83] tr loss: 0.001\n",
            "[83] va loss: 4.180\n",
            "83 78\n",
            "[84] tr loss: 0.001\n",
            "[84] va loss: 4.166\n",
            "83 78\n",
            "[85] tr loss: 0.001\n",
            "[85] va loss: 4.154\n",
            "83 78\n",
            "[86] tr loss: 0.001\n",
            "[86] va loss: 4.142\n",
            "83 78\n",
            "[87] tr loss: 0.001\n",
            "[87] va loss: 4.129\n",
            "83 78\n",
            "[88] tr loss: 0.001\n",
            "[88] va loss: 4.118\n",
            "83 78\n",
            "[89] tr loss: 0.001\n",
            "[89] va loss: 4.100\n",
            "83 78\n",
            "[90] tr loss: 0.001\n",
            "[90] va loss: 4.088\n",
            "83 78\n",
            "[91] tr loss: 0.001\n",
            "[91] va loss: 4.075\n",
            "83 78\n",
            "[92] tr loss: 0.001\n",
            "[92] va loss: 4.061\n",
            "83 78\n",
            "[93] tr loss: 0.001\n",
            "[93] va loss: 4.048\n",
            "83 78\n",
            "[94] tr loss: 0.001\n",
            "[94] va loss: 4.037\n",
            "83 78\n",
            "[95] tr loss: 0.002\n",
            "[95] va loss: 4.025\n",
            "83 78\n",
            "[96] tr loss: 0.001\n",
            "[96] va loss: 3.991\n",
            "83 78\n",
            "[97] tr loss: 0.001\n",
            "[97] va loss: 3.973\n",
            "83 78\n",
            "[98] tr loss: 0.001\n",
            "[98] va loss: 3.966\n",
            "83 78\n",
            "[99] tr loss: 0.001\n",
            "[99] va loss: 3.955\n",
            "83 78\n",
            "[100] tr loss: 0.001\n",
            "[100] va loss: 3.941\n",
            "83 78\n",
            "[101] tr loss: 0.002\n",
            "[101] va loss: 3.930\n",
            "83 78\n",
            "[102] tr loss: 0.001\n",
            "[102] va loss: 3.914\n",
            "83 78\n",
            "[103] tr loss: 0.001\n",
            "[103] va loss: 3.899\n",
            "83 78\n",
            "[104] tr loss: 0.002\n",
            "[104] va loss: 3.887\n",
            "83 78\n",
            "[105] tr loss: 0.001\n",
            "[105] va loss: 3.871\n",
            "83 78\n",
            "[106] tr loss: 0.002\n",
            "[106] va loss: 3.861\n",
            "83 78\n",
            "[107] tr loss: 0.002\n",
            "[107] va loss: 3.844\n",
            "83 78\n",
            "[108] tr loss: 0.002\n",
            "[108] va loss: 3.831\n",
            "83 78\n",
            "[109] tr loss: 0.002\n",
            "[109] va loss: 3.822\n",
            "83 78\n",
            "[110] tr loss: 0.002\n",
            "[110] va loss: 3.802\n",
            "83 78\n",
            "[111] tr loss: 0.002\n",
            "[111] va loss: 3.788\n",
            "83 78\n",
            "[112] tr loss: 0.002\n",
            "[112] va loss: 3.771\n",
            "83 78\n",
            "[113] tr loss: 0.002\n",
            "[113] va loss: 3.770\n",
            "83 78\n",
            "[114] tr loss: 0.002\n",
            "[114] va loss: 3.754\n",
            "83 78\n",
            "[115] tr loss: 0.002\n",
            "[115] va loss: 3.729\n",
            "83 78\n",
            "[116] tr loss: 0.002\n",
            "[116] va loss: 3.714\n",
            "83 78\n",
            "[117] tr loss: 0.002\n",
            "[117] va loss: 3.696\n",
            "83 78\n",
            "[118] tr loss: 0.002\n",
            "[118] va loss: 3.680\n",
            "83 78\n",
            "[119] tr loss: 0.002\n",
            "[119] va loss: 3.663\n",
            "83 78\n",
            "[120] tr loss: 0.002\n",
            "[120] va loss: 3.647\n",
            "83 78\n",
            "[121] tr loss: 0.002\n",
            "[121] va loss: 3.632\n",
            "83 78\n",
            "[122] tr loss: 0.002\n",
            "[122] va loss: 3.626\n",
            "83 78\n",
            "[123] tr loss: 0.002\n",
            "[123] va loss: 3.603\n",
            "83 78\n",
            "[124] tr loss: 0.002\n",
            "[124] va loss: 3.582\n",
            "83 78\n",
            "[125] tr loss: 0.002\n",
            "[125] va loss: 3.563\n",
            "83 78\n",
            "[126] tr loss: 0.002\n",
            "[126] va loss: 3.551\n",
            "83 78\n",
            "[127] tr loss: 0.002\n",
            "[127] va loss: 3.532\n",
            "83 78\n",
            "[128] tr loss: 0.002\n",
            "[128] va loss: 3.515\n",
            "83 78\n",
            "[129] tr loss: 0.002\n",
            "[129] va loss: 3.498\n",
            "83 78\n",
            "[130] tr loss: 0.002\n",
            "[130] va loss: 3.491\n",
            "83 78\n",
            "[131] tr loss: 0.002\n",
            "[131] va loss: 3.460\n",
            "83 78\n",
            "[132] tr loss: 0.002\n",
            "[132] va loss: 3.446\n",
            "83 78\n",
            "[133] tr loss: 0.002\n",
            "[133] va loss: 3.438\n",
            "83 78\n",
            "[134] tr loss: 0.002\n",
            "[134] va loss: 3.420\n",
            "83 78\n",
            "[135] tr loss: 0.002\n",
            "[135] va loss: 3.402\n",
            "83 78\n",
            "[136] tr loss: 0.003\n",
            "[136] va loss: 3.392\n",
            "83 78\n",
            "[137] tr loss: 0.002\n",
            "[137] va loss: 3.396\n",
            "83 78\n",
            "[138] tr loss: 0.002\n",
            "[138] va loss: 3.364\n",
            "83 78\n",
            "[139] tr loss: 0.002\n",
            "[139] va loss: 3.352\n",
            "83 78\n",
            "[140] tr loss: 0.002\n",
            "[140] va loss: 3.328\n",
            "83 78\n",
            "[141] tr loss: 0.002\n",
            "[141] va loss: 3.305\n",
            "83 78\n",
            "[142] tr loss: 0.002\n",
            "[142] va loss: 3.289\n",
            "83 78\n",
            "[143] tr loss: 0.002\n",
            "[143] va loss: 3.280\n",
            "83 78\n",
            "[144] tr loss: 0.002\n",
            "[144] va loss: 3.257\n",
            "83 78\n",
            "[145] tr loss: 0.002\n",
            "[145] va loss: 3.232\n",
            "83 78\n",
            "[146] tr loss: 0.002\n",
            "[146] va loss: 3.220\n",
            "83 78\n",
            "[147] tr loss: 0.002\n",
            "[147] va loss: 3.211\n",
            "83 78\n",
            "[148] tr loss: 0.002\n",
            "[148] va loss: 3.197\n",
            "83 78\n",
            "[149] tr loss: 0.002\n",
            "[149] va loss: 3.178\n",
            "83 78\n",
            "[150] tr loss: 0.002\n",
            "[150] va loss: 3.160\n",
            "83 78\n",
            "[151] tr loss: 0.002\n",
            "[151] va loss: 3.152\n",
            "83 78\n",
            "[152] tr loss: 0.002\n",
            "[152] va loss: 3.139\n",
            "83 78\n",
            "[153] tr loss: 0.003\n",
            "[153] va loss: 3.118\n",
            "83 78\n",
            "[154] tr loss: 0.002\n",
            "[154] va loss: 3.115\n",
            "83 78\n",
            "[155] tr loss: 0.004\n",
            "[155] va loss: 3.098\n",
            "83 78\n",
            "[156] tr loss: 0.003\n",
            "[156] va loss: 3.118\n",
            "83 78\n",
            "[157] tr loss: 0.002\n",
            "[157] va loss: 3.061\n",
            "83 78\n",
            "[158] tr loss: 0.002\n",
            "[158] va loss: 3.035\n",
            "83 78\n",
            "[159] tr loss: 0.002\n",
            "[159] va loss: 3.027\n",
            "83 78\n",
            "[160] tr loss: 0.002\n",
            "[160] va loss: 3.021\n",
            "83 78\n",
            "[161] tr loss: 0.003\n",
            "[161] va loss: 2.994\n",
            "83 78\n",
            "[162] tr loss: 0.003\n",
            "[162] va loss: 2.980\n",
            "83 78\n",
            "[163] tr loss: 0.003\n",
            "[163] va loss: 2.977\n",
            "83 78\n",
            "[164] tr loss: 0.003\n",
            "[164] va loss: 2.950\n",
            "83 78\n",
            "[165] tr loss: 0.003\n",
            "[165] va loss: 2.943\n",
            "83 78\n",
            "[166] tr loss: 0.003\n",
            "[166] va loss: 2.909\n",
            "83 78\n",
            "[167] tr loss: 0.003\n",
            "[167] va loss: 2.925\n",
            "83 78\n",
            "[168] tr loss: 0.003\n",
            "[168] va loss: 2.906\n",
            "83 78\n",
            "[169] tr loss: 0.003\n",
            "[169] va loss: 2.887\n",
            "83 78\n",
            "[170] tr loss: 0.003\n",
            "[170] va loss: 2.874\n",
            "83 78\n",
            "[171] tr loss: 0.003\n",
            "[171] va loss: 2.855\n",
            "83 78\n",
            "[172] tr loss: 0.003\n",
            "[172] va loss: 2.857\n",
            "83 78\n",
            "[173] tr loss: 0.003\n",
            "[173] va loss: 2.840\n",
            "83 78\n",
            "[174] tr loss: 0.003\n",
            "[174] va loss: 2.816\n",
            "83 78\n",
            "[175] tr loss: 0.003\n",
            "[175] va loss: 2.807\n",
            "83 78\n",
            "[176] tr loss: 0.003\n",
            "[176] va loss: 2.796\n",
            "83 78\n",
            "[177] tr loss: 0.004\n",
            "[177] va loss: 2.769\n",
            "83 78\n",
            "[178] tr loss: 0.003\n",
            "[178] va loss: 2.749\n",
            "83 78\n",
            "[179] tr loss: 0.003\n",
            "[179] va loss: 2.746\n",
            "83 78\n",
            "[180] tr loss: 0.003\n",
            "[180] va loss: 2.712\n",
            "83 78\n",
            "[181] tr loss: 0.003\n",
            "[181] va loss: 2.729\n",
            "83 78\n",
            "[182] tr loss: 0.003\n",
            "[182] va loss: 2.706\n",
            "83 78\n",
            "[183] tr loss: 0.003\n",
            "[183] va loss: 2.696\n",
            "83 78\n",
            "[184] tr loss: 0.003\n",
            "[184] va loss: 2.686\n",
            "83 78\n",
            "[185] tr loss: 0.003\n",
            "[185] va loss: 2.670\n",
            "83 78\n",
            "[186] tr loss: 0.003\n",
            "[186] va loss: 2.662\n",
            "83 78\n",
            "[187] tr loss: 0.003\n",
            "[187] va loss: 2.638\n",
            "83 78\n",
            "[188] tr loss: 0.004\n",
            "[188] va loss: 2.637\n",
            "83 78\n",
            "[189] tr loss: 0.003\n",
            "[189] va loss: 2.596\n",
            "83 78\n",
            "[190] tr loss: 0.003\n",
            "[190] va loss: 2.619\n",
            "83 78\n",
            "[191] tr loss: 0.003\n",
            "[191] va loss: 2.591\n",
            "83 78\n",
            "[192] tr loss: 0.004\n",
            "[192] va loss: 2.568\n",
            "83 78\n",
            "[193] tr loss: 0.004\n",
            "[193] va loss: 2.586\n",
            "83 78\n",
            "[194] tr loss: 0.003\n",
            "[194] va loss: 2.559\n",
            "83 78\n",
            "[195] tr loss: 0.004\n",
            "[195] va loss: 2.555\n",
            "83 78\n",
            "[196] tr loss: 0.004\n",
            "[196] va loss: 2.534\n",
            "83 78\n",
            "[197] tr loss: 0.004\n",
            "[197] va loss: 2.543\n",
            "83 78\n",
            "[198] tr loss: 0.004\n",
            "[198] va loss: 2.514\n",
            "83 78\n",
            "[199] tr loss: 0.004\n",
            "[199] va loss: 2.517\n",
            "83 78\n",
            "[200] tr loss: 0.004\n",
            "[200] va loss: 2.497\n",
            "83 78\n",
            "[201] tr loss: 0.004\n",
            "[201] va loss: 2.489\n",
            "83 78\n",
            "[202] tr loss: 0.004\n",
            "[202] va loss: 2.432\n",
            "83 78\n",
            "[203] tr loss: 0.004\n",
            "[203] va loss: 2.469\n",
            "83 78\n",
            "[204] tr loss: 0.004\n",
            "[204] va loss: 2.431\n",
            "83 78\n",
            "[205] tr loss: 0.004\n",
            "[205] va loss: 2.420\n",
            "83 78\n",
            "[206] tr loss: 0.005\n",
            "[206] va loss: 2.386\n",
            "83 78\n",
            "[207] tr loss: 0.004\n",
            "[207] va loss: 2.440\n",
            "83 78\n",
            "[208] tr loss: 0.004\n",
            "[208] va loss: 2.327\n",
            "83 78\n",
            "[209] tr loss: 0.004\n",
            "[209] va loss: 2.387\n",
            "83 78\n",
            "[210] tr loss: 0.004\n",
            "[210] va loss: 2.320\n",
            "83 78\n",
            "[211] tr loss: 0.004\n",
            "[211] va loss: 2.356\n",
            "83 78\n",
            "[212] tr loss: 0.004\n",
            "[212] va loss: 2.305\n",
            "83 78\n",
            "[213] tr loss: 0.004\n",
            "[213] va loss: 2.426\n",
            "83 78\n",
            "[214] tr loss: 0.005\n",
            "[214] va loss: 2.332\n",
            "83 78\n",
            "[215] tr loss: 0.005\n",
            "[215] va loss: 2.173\n",
            "83 78\n",
            "[216] tr loss: 0.005\n",
            "[216] va loss: 2.287\n",
            "83 78\n",
            "[217] tr loss: 0.005\n",
            "[217] va loss: 2.223\n",
            "83 78\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora entrenamos el random_forest"
      ],
      "metadata": {
        "id": "fHL_d2UN0RBY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "source_df = pd.read_csv('results.csv')\n",
        "treeclass = Customtree(source_df)\n",
        "#treeclass.write_dict_to_file('params_tree/treeparam.txt')\n",
        "train_tree(treeclass,'params_tree/treeparam.txt')\n"
      ],
      "metadata": {
        "id": "Hgku391usRVr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}